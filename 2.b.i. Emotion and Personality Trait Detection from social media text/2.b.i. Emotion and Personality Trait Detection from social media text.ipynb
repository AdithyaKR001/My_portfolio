{"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","interpreter":{"hash":"c4f292dd4e491f951fa1ae892a0079fd60d7f576c87c673db8bd2b1ea9bc27d0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport re\nimport nltk\nimport string\nimport tensorflow as tf\nimport keras\nimport warnings\nimport gensim.downloader as api\nimport io\nwarnings.filterwarnings('ignore')\n# from keras_preprocessing.sequence import pad_sequences\n# from keras.preprocessing.sequence import pad_sequences\nfrom sklearn.preprocessing import LabelEncoder\n# from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\n# from parallel_pandas import ParallelPandas\n#initialize parallel-pandas\n# ParallelPandas.initialize(n_cpu=16, split_factor=4, disable_pr_bar=False)\n# from keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import f1_score,classification_report,accuracy_score\nfrom sklearn.metrics import precision_score,recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Conv1D, MaxPooling1D, LSTM, Bidirectional, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom tensorflow.keras.models import Model\n# from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint, ReduceLROnPlateau\nfrom tensorflow.keras.layers import BatchNormalization,Activation\nfrom tensorflow.keras.layers import GRU","metadata":{"id":"a4elf4sS27E9","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.config.set_soft_device_placement(False)\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import  GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import  MaxPool1D, Flatten","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.constraints import MaxNorm","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.utils import class_weight","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow.keras.utils","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install -U numpy==1.11.0.","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyValidationError(Exception):\n    pass\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('all')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from tensorflow.python.compiler.mlcompute import mlcompute\n\n# tf.compat.v1.disable_eager_execution()\n# mlcompute.set_mlc_device(device_name='gpu')\n# from tensorflow.mlcompute import mlcompute\n# !python -m numpy","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.config.list_physical_devices())","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"F25XXQV_27E_","outputId":"b057bcec-91a3-4a06-ba96-9a095799f015","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(np.__version__)\n# !pip install mlcompute\n# !pip install tensorflow-gpu\n# !pip install parallel-pandas\n# !pip install tensorflow\n# !pip install nltk\n# !pip install gensim\n# !python -m pip install sklearn\n# !pip install -U scikit-learn scipy matplotlib","metadata":{"id":"Jcx_QUIZ27E_","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\nstop_words = stopwords.words('english')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions for data pre-processing\ndef toLowerCase(text):\n#     print(\"Started function toLowerCase\")\n    return str.lower(text)\n\ndef remove_html(text):\n#     print(\"Started function remove_html\")\n    new_soup = BeautifulSoup(text,'html.parser')\n    text = new_soup.get_text()\n    return text\n\ndef remove_punctuation(text):\n#     print(\"Started function remove_punctuation\")\n    text = \" \".join([word for word in text if word not in string.punctuation])\n    return text\n\ndef url_replace(text):\n#     print(\"Started function url_replace\")\n#     re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text)\n    matches = re.findall(r'http\\S+', text)\n#     print(matches)\n\n    for match in matches:\n        text = text.replace(match, '')\n    \n    matches = re.findall(r'www.\\S+', text)    \n    \n    for match in matches:\n        text = text.replace(match, '')\n    \n    return text\n\ndef whiteSpaceRemoval(text):\n#     print(\"Started function whiteSpaceRemoval\")\n    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n    return text\n\ndef specialCharacterRemoval(text):\n#     print(\"Started function specialCharacterRemoval\")\n    text = re.sub(r'\\W', ' ', str(text))\n    return text\n\ndef singleCharRemoval(text):\n#     print(\"Started function singleCharRemoval\")\n    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n    return text\n\ndef numbers_cleanup(x):\n#     print(\"Started function numbers_cleanup\")\n    if bool(re.search(r'\\d', x)):\n        x = re.sub('[0-9]{5,}', '#####', x)\n        x = re.sub('[0-9]{4}', '####', x)\n        x = re.sub('[0-9]{3}', '###', x)\n        x = re.sub('[0-9]{2}', '##', x)\n    return x\n\ndef lemmatizeStopWordAndSmallWordRemoval(text):\n#     print(\"Started function lemmatizeStopWordAndSmallWordRemoval\")\n    words = text.split()\n    words_lemmatized = [lemmatizer.lemmatize(word=word) for word in words]\n    words_lemmatized_nostop = [word for word in words_lemmatized if word not in stop_words]\n    # for word in words_lemmatized:\n    #     if word not in stopwords.words():\n    #         words_lemmatized_nostop.append(word)\n    # words_lemmatized_nostop = [word for word.words() in words_lemmatized if word not in stopwords]\n    word_lemmatized_nostop_lessthan3 = [word for word in words_lemmatized_nostop if len(word)>3]\n    text = ' '.join(word_lemmatized_nostop_lessthan3)\n    return text\n    \n    ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define a function to compute the max length of sequence\ndef max_length(sequences):\n    '''\n    input:\n        sequences: a 2D list of integer sequences\n    output:\n        max_length: the max length of the sequences\n    '''\n    max_length = 0\n    for i, seq in enumerate(sequences):\n        length = len(seq)\n        if max_length < length:\n            max_length = length\n    return max_length","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preProcessDataMBTI(df_mbti,approach):\n    print(\"Started execution of function preProcessDataMBTI\")\n    print(\"Number of columns:\",df_mbti.columns)\n    print(\"Number of records:\",len(df_mbti))\n    print(\"Number of records with personality type assigned: \",len(df_mbti['type'].notnull()))\n    df_mbti['len'] = df_mbti['posts'].apply(lambda s : len(s))\n#     df_mbti['len'].plot.hist(bins=100)\n    maximum_len1 = int(df_mbti.len.quantile(0.9))\n    lemmatizer = WordNetLemmatizer()\n    stop_words = stopwords.words('english')\n    print(\"Class-wise break-up of personality types:\\n\",df_mbti.type.value_counts())\n    with tf.device(\"/TPU:0\"):\n        df_mbti['posts'] = df_mbti[\"posts\"].apply(lambda x: toLowerCase(x))\n        df_mbti['posts'] = df_mbti['posts'].apply(lambda x: remove_html(x))\n        df_mbti['posts'] = df_mbti['posts'].apply(lambda x: url_replace(x))\n        df_mbti['posts'] = df_mbti['posts'].apply(lambda x: word_tokenize(x))\n        df_mbti['posts'] = df_mbti[\"posts\"].apply(lambda x: specialCharacterRemoval(x))\n        # df_mbti['posts'] = df_mbti[\"posts\"].apply(lambda x: remove_punctuation(x))\n        df_mbti['posts'] = df_mbti[\"posts\"].apply(lambda x: numbers_cleanup(x))\n        df_mbti['posts'] = df_mbti[\"posts\"].apply(lambda x: whiteSpaceRemoval(x))\n        df_mbti['posts'] = df_mbti[\"posts\"].apply(lambda x: singleCharRemoval(x))\n        df_mbti['posts'] = df_mbti[\"posts\"].apply(lambda x: lemmatizeStopWordAndSmallWordRemoval(x))\n        \n#         size_vocabulary = 10000\n        token_oov = '<OOV>'\n        trunc_type='post'\n        padding_type='post'\n        embedding_dimension = 300\n        x_train, x_test, y_train, y_test = train_test_split(df_mbti['posts'], df_mbti['type'],\n                                                        stratify=df_mbti['type'], \n                                                        test_size=0.30)\n\n        x_test, x_holdout, y_test, y_holdout = train_test_split(x_test, y_test,\n                                                        stratify=y_test, \n                                                        test_size=0.33)\n    #     text_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = size_vocabulary, oov_token=token_oov)\n        class_weights_train = class_weight.compute_class_weight(class_weight ='balanced',classes=np.unique(y_train),y=y_train)\n        class_weights_test = class_weight.compute_class_weight(class_weight ='balanced',classes=np.unique(y_test),y=y_test)\n        class_weights_train = dict(enumerate(class_weights_train))\n        class_weights_test = dict(enumerate(class_weights_test))\n    #     class_weights = class_weight.compute_class_weight(class_weight ='balanced',classes=np.unique(y_train),y=y_train)\n    #     class_weights = {l:c for l,c in zip(np.unique(y_train), class_weights)}\n    #     class_weights = dict(enumerate(class_weights))\n    \n        text_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=token_oov)#,num_words=size_vocabulary)\n        text_tokenizer.fit_on_texts(x_train)\n        \n        print(\"Sample tokens \\n\",dict(list(text_tokenizer.word_index.items())[:15]))\n        \n        x_train_sequences = text_tokenizer.texts_to_sequences(x_train)\n        if approach is 'new':\n            maximum_len2 = max_length(x_train_sequences)\n            maximum_len = min(maximum_len1,maximum_len2)\n        else:\n            maximum_len = maximum_len1\n    #     print(\"Tokenized training data sample: \\n\",x_train_sequences[5])\n        x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train_sequences,maxlen=maximum_len,\n                                                                            truncating=trunc_type,\n                                                                            padding=padding_type)\n        print(\"Sample training data padded length is: \",len(x_train_padded[5]))\n        x_test_sequences = text_tokenizer.texts_to_sequences(x_test)\n        x_test_padded = tf.keras.preprocessing.sequence.pad_sequences(x_test_sequences,maxlen=maximum_len,\n                                                                            truncating=trunc_type,\n                                                                            padding=padding_type)\n        x_holdout_sequences = text_tokenizer.texts_to_sequences(x_holdout)\n        x_holdout_padded = tf.keras.preprocessing.sequence.pad_sequences(x_holdout_sequences,maxlen=maximum_len,\n                                                                            truncating=trunc_type,\n                                                                            padding=padding_type)\n        label_encoder = LabelEncoder()\n        label_encoder.fit(y_train)\n        y_train_encoded = label_encoder.transform(y_train.values)\n        y_test_encoded = label_encoder.transform(y_test.values)\n        y_holdout_encoded = label_encoder.transform(y_holdout.values)\n\n    #     if approach is 'new':\n    #         y_train_encoded = tensorflow.keras.utils.to_categorical(y_train_encoded)\n    #         y_test_encoded = tensorflow.keras.utils.to_categorical(y_test_encoded)\n    #         y_holdout_encoded = tensorflow.keras.utils.to_categorical(y_holdout_encoded)\n\n        print(\"Shape of training data with padding: \", x_train_padded.shape)\n        print(\"Shape of testing data with padding: \", x_test_padded.shape)\n        print(\"Shape of holdout data with padding: \", x_holdout_padded.shape)\n        print(\"Shape of training labels encoded: \", y_train_encoded.shape)\n        print(\"Shape of testing labels encoded: \", y_test_encoded.shape)\n        print(\"Shape of holdout labels encoded: \", y_holdout_encoded.shape)\n        num_labels = len(df_mbti['type'].unique())\n        print(\"Number of labels: \",num_labels)\n\n        return x_train_padded, x_test_padded, x_holdout_padded, y_train_encoded, y_test_encoded, y_holdout_encoded, text_tokenizer, maximum_len, embedding_dimension, label_encoder, num_labels, class_weights_train, class_weights_test","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def removeColons(text):\n    text = text.replace(':','')\n    return str(text)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preProcessDataTEC(df_tec,approach):\n    print(\"Started execution of function preProcessDataTEC\")\n#     df_tec = df_tec.rename(columns={0:\"tweets\",1:\"emotions\"},errors=\"raise\")\n    df_tec[\"tweets\"] = df_tec[\"tweets\"].astype('str')\n    df_tec[\"emotions\"] = df_tec[\"emotions\"].astype('str')    \n    df_tec['emotions'] = df_tec['emotions'].apply(lambda x: removeColons(x))    \n    print(\"Number of columns:\",df_tec.columns)\n    print(\"Number of records:\",len(df_tec))\n    print(\"Number of records with emotion assigned: \",len(df_tec['emotions'].notnull()))\n    print(df_tec.dtypes)\n    df_tec['len'] = df_tec['tweets'].apply(lambda s : len(s))\n#     df_tec['len'].plot.hist(bins=100)\n    maximum_len1 = int(df_tec.len.quantile(0.9))\n    \n    lemmatizer = WordNetLemmatizer()\n    stop_words = stopwords.words('english')\n    print(\"Class-wise break-up of emotions:\\n\",df_tec.emotions.value_counts())\n    with tf.device(\"/TPU:0\"):\n        df_tec['tweets'] = df_tec[\"tweets\"].apply(lambda x: toLowerCase(x))\n        df_tec['tweets'] = df_tec['tweets'].apply(lambda x: remove_html(x))\n        df_tec['tweets'] = df_tec['tweets'].apply(lambda x: url_replace(x))\n        df_tec['tweets'] = df_tec['tweets'].apply(lambda x: word_tokenize(x))\n        df_tec['tweets'] = df_tec[\"tweets\"].apply(lambda x: specialCharacterRemoval(x))\n        # df_tec['tweets'] = df_tec[\"tweets\"].apply(lambda x: remove_punctuation(x))\n        df_tec['tweets'] = df_tec[\"tweets\"].apply(lambda x: numbers_cleanup(x))\n        df_tec['tweets'] = df_tec[\"tweets\"].apply(lambda x: whiteSpaceRemoval(x))\n        df_tec['tweets'] = df_tec[\"tweets\"].apply(lambda x: singleCharRemoval(x))\n        df_tec['tweets'] = df_tec[\"tweets\"].apply(lambda x: lemmatizeStopWordAndSmallWordRemoval(x))\n        \n        \n        token_oov = '<OOV>'\n        trunc_type='post'\n        padding_type='post'\n        embedding_dimension = 300\n        x_train, x_test, y_train, y_test = train_test_split(df_tec['tweets'], df_tec['emotions'],\n                                                        stratify=df_tec['emotions'], \n                                                        test_size=0.30)\n\n        x_test, x_holdout, y_test, y_holdout = train_test_split(x_test, y_test,\n                                                        stratify=y_test, \n                                                        test_size=0.33)\n    #     text_tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = size_vocabulary, oov_token=token_oov)\n    \n        class_weights_train = class_weight.compute_class_weight(class_weight ='balanced',classes=np.unique(y_train),y=y_train)\n        class_weights_test = class_weight.compute_class_weight(class_weight ='balanced',classes=np.unique(y_test),y=y_test)\n        class_weights_train = dict(enumerate(class_weights_train))\n        class_weights_test = dict(enumerate(class_weights_test))\n        \n        text_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=token_oov)\n        text_tokenizer.fit_on_texts(x_train)\n        \n        print(\"Sample tokens \\n\",dict(list(text_tokenizer.word_index.items())[:15]))\n        x_train_sequences = text_tokenizer.texts_to_sequences(x_train)\n        if approach is 'new':\n            maximum_len2 = max_length(x_train_sequences)\n            maximum_len = min(maximum_len1,maximum_len2)\n        else:\n            maximum_len = maximum_len1\n    \n        x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train_sequences,maxlen=maximum_len,\n                                                                           truncating=trunc_type,\n                                                                           padding=padding_type)\n        print(\"Sample training data padded length: \",len(x_train_padded[5]))\n        x_test_sequences = text_tokenizer.texts_to_sequences(x_test)\n        x_test_padded = tf.keras.preprocessing.sequence.pad_sequences(x_test_sequences,maxlen=maximum_len,\n                                                                            truncating=trunc_type,\n                                                                            padding=padding_type)\n        x_holdout_sequences = text_tokenizer.texts_to_sequences(x_holdout)\n        x_holdout_padded = tf.keras.preprocessing.sequence.pad_sequences(x_holdout_sequences,maxlen=maximum_len,\n                                                                            truncating=trunc_type,\n                                                                            padding=padding_type)\n        label_encoder = LabelEncoder()\n        label_encoder.fit(y_train)\n        y_train_encoded = label_encoder.transform(y_train.values)\n        y_test_encoded = label_encoder.transform(y_test.values)\n        y_holdout_encoded = label_encoder.transform(y_holdout.values)\n        \n#         y_train_encoded = tensorflow.keras.utils.to_categorical(y_train_encoded)\n#         y_test_encoded = tensorflow.keras.utils.to_categorical(y_test_encoded)\n#         y_holdout_encoded = tensorflow.keras.utils.to_categorical(y_holdout_encoded)\n\n        print(\"Shape of training data with padding: \", x_train_padded.shape)\n        print(\"Shape of testing data with padding: \", x_test_padded.shape)\n        print(\"Shape of holdout data with padding: \", x_holdout_padded.shape)\n        print(\"Shape of training labels encoded: \", y_train_encoded.shape)\n        print(\"Shape of testing labels encoded: \", y_test_encoded.shape)\n        print(\"Shape of holdout labels encoded: \", y_holdout_encoded.shape)\n        num_labels = len(df_tec['emotions'].unique())\n        print(\"Number of labels: \",num_labels)\n\n        return x_train_padded, x_test_padded, x_holdout_padded, y_train_encoded, y_test_encoded, y_holdout_encoded, text_tokenizer, maximum_len, embedding_dimension, label_encoder, num_labels, class_weights_train, class_weights_test","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preProcessDataISEAR(df_isear,approach):\n    print(\"Started execution of function preProcessDataISEAR\")\n    df_isear = df_isear.rename(columns={\"Field1\":\"emotions\",\"SIT\":\"answers\"},errors=\"raise\")\n#     df_isear['emotions'] = df_isear['emotions'].apply(lambda x: removeColons(x))    \n    df_isear['emotions'] = df_isear['emotions'].astype('str')\n    df_isear['answers'] = df_isear['answers'].astype('str')\n    print(\"Number of columns:\",df_isear.columns)\n    print(\"Number of records:\",len(df_isear))\n    print(\"Number of records with emotion assigned: \",len(df_isear['emotions'].notnull()))\n    df_isear['len'] = df_isear['answers'].apply(lambda s : len(s))\n#     df_isear['len'].plot.hist(bins=100)\n    maximum_len1 = int(df_isear.len.quantile(0.9))\n    lemmatizer = WordNetLemmatizer()\n    stop_words = stopwords.words('english')\n    \n    print(\"Class-wise break-up of emotions:\\n\",df_isear.emotions.value_counts())\n    with tf.device(\"/TPU:0\"):\n        df_isear['answers'] = df_isear[\"answers\"].apply(lambda x: toLowerCase(x))\n        df_isear['answers'] = df_isear['answers'].apply(lambda x: remove_html(x))\n        df_isear['answers'] = df_isear['answers'].apply(lambda x: url_replace(x))\n        df_isear['answers'] = df_isear['answers'].apply(lambda x: word_tokenize(x))\n        df_isear['answers'] = df_isear[\"answers\"].apply(lambda x: specialCharacterRemoval(x))\n        # df_isear['answers'] = df_isear[\"answers\"].apply(lambda x: remove_punctuation(x))\n        df_isear['answers'] = df_isear[\"answers\"].apply(lambda x: numbers_cleanup(x))\n        df_isear['answers'] = df_isear[\"answers\"].apply(lambda x: whiteSpaceRemoval(x))\n        df_isear['answers'] = df_isear[\"answers\"].apply(lambda x: singleCharRemoval(x))\n        df_isear['answers'] = df_isear[\"answers\"].apply(lambda x: lemmatizeStopWordAndSmallWordRemoval(x))\n    \n        token_oov = '<OOV>'\n        trunc_type='post'\n        padding_type='post'\n        embedding_dimension = 300\n\n        x_train, x_test, y_train, y_test = train_test_split(df_isear['answers'], df_isear['emotions'],\n                                                        stratify=df_isear['emotions'], \n                                                        test_size=0.30)\n\n        x_test, x_holdout, y_test, y_holdout = train_test_split(x_test, y_test,\n                                                        stratify=y_test, \n                                                        test_size=0.33)\n\n        class_weights_train = class_weight.compute_class_weight(class_weight ='balanced',classes=np.unique(y_train),y=y_train)\n        class_weights_test = class_weight.compute_class_weight(class_weight ='balanced',classes=np.unique(y_test),y=y_test)\n        class_weights_train = dict(enumerate(class_weights_train))\n        class_weights_test = dict(enumerate(class_weights_test))\n\n        text_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=token_oov)\n        text_tokenizer.fit_on_texts(x_train)\n        print(\"Sample tokens \\n\",dict(list(text_tokenizer.word_index.items())[:15]))\n        x_train_sequences = text_tokenizer.texts_to_sequences(x_train)\n\n        if approach is 'new':\n            maximum_len2 = max_length(x_train_sequences)\n            maximum_len = min(maximum_len1,maximum_len2)\n        else:\n            maximum_len = maximum_len1\n\n        x_train_padded = tf.keras.preprocessing.sequence.pad_sequences(x_train_sequences,maxlen=maximum_len,\n                                                                           truncating=trunc_type,\n                                                                           padding=padding_type)\n\n        print(\"Sample training data padded length: \",len(x_train_padded[5]))\n\n        x_test_sequences = text_tokenizer.texts_to_sequences(x_test)\n        x_test_padded = tf.keras.preprocessing.sequence.pad_sequences(x_test_sequences,maxlen=maximum_len, padding=padding_type,truncating=trunc_type)\n\n        x_holdout_sequences = text_tokenizer.texts_to_sequences(x_holdout)\n        x_holdout_padded = tf.keras.preprocessing.sequence.pad_sequences(x_holdout_sequences,maxlen=maximum_len, padding=padding_type,truncating=trunc_type)\n\n        label_encoder = LabelEncoder()\n        label_encoder.fit(y_train.values)\n        y_train_encoded = label_encoder.transform(y_train.values)\n        y_test_encoded = label_encoder.transform(y_test.values)\n        y_holdout_encoded = label_encoder.transform(y_holdout.values)\n\n    #     y_train_encoded = tensorflow.keras.utils.to_categorical(y_train_encoded)\n    #     y_test_encoded = tensorflow.keras.utils.to_categorical(y_test_encoded)\n    #     y_holdout_encoded = tensorflow.keras.utils.to_categorical(y_holdout_encoded)\n\n        print(\"Shape of training data with padding: \", x_train_padded.shape)\n        print(\"Shape of testing data with padding: \", x_test_padded.shape)\n        print(\"Shape of holdout data with padding: \", x_holdout_padded.shape)\n        print(\"Shape of training labels encoded: \", y_train_encoded.shape)\n        print(\"Shape of testing labels encoded: \", y_test_encoded.shape)\n        print(\"Shape of holdout labels encoded: \", y_holdout_encoded.shape)\n        num_labels = len(df_isear['emotions'].unique())\n        print(\"Number of labels: \",num_labels)\n\n        return x_train_padded, x_test_padded, x_holdout_padded, y_train_encoded, y_test_encoded, y_holdout_encoded, text_tokenizer, maximum_len, embedding_dimension, label_encoder, num_labels, class_weights_train, class_weights_test\n    ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def readDataAndPreprocess(datasetName,approach):\n    print(\"Started function exeuction of readDataAndPreprocess\")\n    if datasetName is 'MBTI':\n        df_data = pd.read_csv('/kaggle/input/mbtinew/mbti_1.csv')#('gdrive/MyDrive/mbti_1.csv')\n        return preProcessDataMBTI(df_data,approach)\n        \n    elif datasetName is 'TEC':\n        # \n        df_data = pd.read_csv('/kaggle/input/kaggleinputtwitteremotioncorpus-1/tec.csv')\n        return preProcessDataTEC(df_data,approach)\n    elif datasetName is 'ISEAR':\n        # \n        df_data = pd.read_excel('/kaggle/input/isearfortextclassification/ISEAR.xlsx')        \n        return preProcessDataISEAR(df_data,approach)\n    else:\n        raise MyValidationError(\"Invalid input dataset name.\")        \n    \n    # return df_data_preprocessed","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_embeddings_glove(dir_glove): \n    print(\"Started execution of function create_embeddings_glove\")\n    embeddings_index = {}\n    f = open(dir_glove,encoding=\"utf8\")\n    for line in f:\n        values = line.split()\n        word = values[0]\n        embeddings_index[word] = np.asarray(values[1:],dtype='float32')\n    f.close()\n\n    return embeddings_index","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\ndef loadFastTextEmbeddings(fname):\n    print(\"Started execution of function loadFastTextEmbeddings\")\n#     fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n#     n, d = map(int, fin.readline().split())\n#     data = {}\n#     for line in fin:\n#         tokens = line.rstrip().split(' ')\n#         data[tokens[0]] = map(float, tokens[1:])\n#     return data\n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    embeddings_fasttext = dict(get_coefs(*o.split(\" \")) for o in open(fname) if len(o)>100)\n    return embeddings_fasttext","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gensim\nfrom gensim.models import Word2Vec, KeyedVectors","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef loadWord2vecEmbeddings(filePath):\n    print(\"Started execution of function loadWord2vecEmbeddings\")\n#     embeddings_index = {}\n#     f = open(filePath,encoding=\"utf8\")\n#     for line in f:\n#         values = line.split()\n#         word = values[0]\n#         embeddings_index[word] = np.asarray(values[1:],dtype='float32')\n#     f.close()\n    \n#     embeddings_index = KeyedVectors.load_word2vec_format(filePath, binary=True, limit=100000)\n    embeddings_index = KeyedVectors.load_word2vec_format(filePath, binary=True)\n    return embeddings_index\n    ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    embeddings_word2vec = loadWord2vecEmbeddings('/kaggle/input/googlenewsword2vec10b300d/GoogleNews-vectors-negative300.bin')\n    embeddings_fasttext = loadFastTextEmbeddings('/kaggle/input/fasttextwikinews300d1msubword/wiki-news-300d-1M-subword.vec')\n    indexed_embeddings_glove = create_embeddings_glove(f\"/kaggle/input/glove42b300/glove.42B.300d.txt\")\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getEmbeddingMatrixForGlove(text_tokenizer, embedding_dimension,indexed_embeddings_glove):\n    print(\"Started function execution of getEmbeddingMatrixForGlove\")\n#     dir_glove = f\"/kaggle/input/glove42b300/glove.42B.300d.txt\"#f\"gdrive/MyDrive/glove.42B.{embedding_dimension}d.txt\" #f\"./glove.42B.{embedding_dimension}d.txt\"\n#     indexed_embeddings_glove = create_embeddings_glove(dir_glove)\n    words_index = text_tokenizer.word_index\n    # embedding_dimension = 300\n    # words_index\n    matrix_embeddings = np.zeros((len(words_index)+1,embedding_dimension))\n    for word,idx in words_index.items():\n        vector_embeddings = indexed_embeddings_glove.get(word)\n        if vector_embeddings is not None:\n            matrix_embeddings[idx] = vector_embeddings\n            \n    print(\"Shape of the word-embeddings matrix: \",matrix_embeddings.shape)\n    return matrix_embeddings\n    # ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# embeddings_fasttext = api.load(\"fasttext-wiki-news-subwords-300\")\n# embeddings_fasttext = api.load(\"fasttext-wiki-news-subwords-300\")\n# /kaggle/input/fasttextwikinews300d1msubword/wiki-news-300d-1M-subword.vec","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getEmbeddingMatrixForFasttext(text_tokenizer, embedding_dimension,embeddings_fasttext):\n    print(\"Started execution of function getEmbeddingMatrixForFasttext\")\n#     embeddings_fasttext = loadFastTextEmbeddings('/kaggle/input/fasttextwikinews300d1msubword/wiki-news-300d-1M-subword.vec')\n\n    words_index = text_tokenizer.word_index\n    # embedding_dimension = 300\n    # words_index\n    print(\"Started creating the embedding matrix.\")\n    matrix_embeddings = np.zeros((len(words_index)+1,embedding_dimension))\n    for word,idx in words_index.items():\n#         if word is '<OOV>':\n#             word = 'OOV'\n#             continue\n        vector_embeddings = embeddings_fasttext.get(word)#[word]\n        if vector_embeddings is not None:\n            matrix_embeddings[idx] = vector_embeddings\n            \n    print(\"Shape of the word-embeddings matrix: \",matrix_embeddings.shape)\n    return matrix_embeddings\n    \n    ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getEmbeddingMatrixForWord2Vec(text_tokenizer, embedding_dimension,embeddings_word2vec):\n    print(\"Started execution of function getEmbeddingMatrixForWord2Vec\")\n#     embeddings_word2vec = loadWord2vecEmbeddings('/kaggle/input/googlenewsword2vec10b300d/GoogleNews-vectors-negative300.bin')#('/kaggle/input/fasttextwikinews300d1msubword/wiki-news-300d-1M-subword.vec')\n\n    words_index = text_tokenizer.word_index\n    # embedding_dimension = 300\n    # words_index\n    print(\"Started creating the embedding matrix.\")\n    matrix_embeddings = np.zeros((len(words_index)+1,embedding_dimension))\n    for word,idx in words_index.items():\n#         if word is '<OOV>':\n#             word = 'OOV'\n#             continue\n        try:\n            vector_embeddings = embeddings_word2vec[word]#.get(word)#[word]\n        except KeyError:\n            continue\n        if vector_embeddings is not None:\n            matrix_embeddings[idx] = vector_embeddings\n            \n    print(\"Shape of the word-embeddings matrix: \",matrix_embeddings.shape)\n    return matrix_embeddings","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createModelCNNBILSTM(num_filters, kernel_size, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer):\n    with tpu_strategy.scope():\n        print(\"Started execution of function createModelCNNBILSTM\")\n        print(\"Number of filters: \",num_filters)\n        print(\"Number of kernels: \",kernel_size)\n        print(\"Optimizer is: \",optimizer)\n    #     if optimizer is 'Adam':\n    #         opt = 'adam'        \n    #     elif optimizer is 'SGD':\n    #         opt = 'sgd'\n    #     elif optimizer is 'adadelta':\n    #         opt = 'adadelta'\n    #     else:\n    #         raise MyValidationError(\"Invalid optimizer name.\")\n\n        # input_sequence = Input(shape=(9050,), dtype='int32')    \n        # layer_embedding = Embedding(len(words_index) + 1,embedding_dimension,weights=[matrix_embeddings],input_length=maximum_len,trainable=False)\n\n        input_sequence = Input(shape=(maximum_len,), dtype='int32')\n        layer_embedding = Embedding(len(text_tokenizer.word_index) + 1,embedding_dimension,weights=[matrix_embeddings],input_length=maximum_len,trainable=False)\n        sequences_embedded = layer_embedding(input_sequence)\n        layer_conv1 = Conv1D(num_filters, kernel_size, activation='relu')(sequences_embedded)\n    #     26th Apr\n        layer_pool1 = MaxPooling1D(pool_size=5,padding='same')(layer_conv1)\n    #     layer_pool1 = MaxPooling1D(pool_size=2,strides=2)(layer_conv1)\n    #  26th Apr\n        layer_drop1 = Dropout(0.5)(layer_pool1)\n        layer_drop1 = Dropout(0.5)(layer_drop1)\n\n    #     layer_drop1 = Dropout(0.5)(layer_drop1)\n        layer_bn1 = BatchNormalization()(layer_drop1)\n\n        layer_conv2 = Conv1D(num_filters, kernel_size, activation='relu')(layer_bn1)\n\n    #     layer_pool2 = MaxPooling1D(pool_size=5)(layer_conv2)\n\n        layer_pool2 = MaxPooling1D(pool_size=5,padding='same')(layer_conv2)\n        layer_drop2 = Dropout(0.5)(layer_pool2)\n        layer_drop2 = Dropout(0.5)(layer_drop2)\n\n    #     layer_drop2 = Dropout(0.5)(layer_drop2)\n\n        layer_bn2 = BatchNormalization()(layer_drop2)\n\n        # layer_conv3 = Conv1D(128, 5, activation='relu')(layer_bn2)\n        # layer_pool3 = MaxPooling1D(pool_size=5)(layer_conv3)\n        # layer_drop3 = Dropout(0.5)(layer_pool3)\n        # layer_bn3 = BatchNormalization()(layer_drop3)\n\n        layer_bilstm = Bidirectional(LSTM(num_filters,return_sequences=False,dropout=0.2,recurrent_dropout=0.2))(layer_bn2)#(layer_drop2)#(layer_pool3)\n\n        # lstm_layer1 = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(layer_bn3)#(pooling_layer2)\n        # lstm_layer2 = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(lstm_layer1)\n    #     layer_drop3 = Dropout(0.5)(layer_bilstm)\n    #     layer_drop3 = Dropout(0.5)(layer_drop3)\n    #     27th Apr\n    #     layer_dense1 = Dense(512,activation='sigmoid')(layer_drop3)\n    #     27th Apr\n\n        layer_drop3 = Dropout(0.5)(layer_bilstm)\n        layer_drop3 = Dropout(0.5)(layer_drop3)\n\n    #     layer_dense1 = Dense(256,activation='relu')(layer_bilstm)\n    #     layer_drop4 = Dropout(0.5)(layer_dense1)\n\n        layer_output = Dense(num_labels,activation='softmax')(layer_drop3)#(layer_dense1)#(layer_drop3)\n\n        # layer_activation = Activation('softmax')(layer_output)\n\n        model_bilstm = Model(inputs=input_sequence, outputs=layer_output)\n        model_bilstm.compile(loss = 'sparse_categorical_crossentropy',metrics=['accuracy'], optimizer=optimizer)\n    return model_bilstm\n    ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createModelCNNBIGRU(num_filters, kernel_size, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer):\n    with tpu_strategy.scope():\n        print(\"Started execution of function createModelCNNBIGRU\")\n        print(\"Number of filters: \",num_filters)\n        print(\"Number of kernels: \",kernel_size)\n        print(\"Optimizer is: \",optimizer)\n    #     if optimizer is 'Adam':\n    #         opt = 'adam'        \n    #     elif optimizer is 'SGD':\n    #         opt = 'sgd'\n    #     elif optimizer is 'adadelta':\n    #         opt = 'adadelta'\n    #     else:\n    #         raise MyValidationError(\"Invalid optimizer name.\")\n\n        # input_sequence = Input(shape=(9050,), dtype='int32')    \n        # layer_embedding = Embedding(len(words_index) + 1,embedding_dimension,weights=[matrix_embeddings],input_length=maximum_len,trainable=False)\n\n        input_sequence = Input(shape=(maximum_len,), dtype='int32')\n        layer_embedding = Embedding(len(text_tokenizer.word_index) + 1,embedding_dimension,weights=[matrix_embeddings],input_length=maximum_len,trainable=False)\n        sequences_embedded = layer_embedding(input_sequence)\n        layer_conv1 = Conv1D(num_filters, kernel_size, activation='relu')(sequences_embedded)\n    #     26th Apr\n        layer_pool1 = MaxPooling1D(pool_size=5,padding='same')(layer_conv1)\n    #     layer_pool1 = MaxPooling1D(pool_size=2,strides=2)(layer_conv1)\n    #  26th Apr\n        layer_drop1 = Dropout(0.5)(layer_pool1)\n        layer_drop1 = Dropout(0.5)(layer_drop1)\n\n    #     layer_drop1 = Dropout(0.5)(layer_drop1)\n        layer_bn1 = BatchNormalization()(layer_drop1)\n\n        layer_conv2 = Conv1D(num_filters, kernel_size, activation='relu')(layer_bn1)\n\n    #     layer_pool2 = MaxPooling1D(pool_size=5)(layer_conv2)\n\n        layer_pool2 = MaxPooling1D(pool_size=5,padding='same')(layer_conv2)\n        layer_drop2 = Dropout(0.5)(layer_pool2)\n        layer_drop2 = Dropout(0.5)(layer_drop2)\n\n    #     layer_drop2 = Dropout(0.5)(layer_drop2)\n\n        layer_bn2 = BatchNormalization()(layer_drop2)\n\n        # layer_conv3 = Conv1D(128, 5, activation='relu')(layer_bn2)\n        # layer_pool3 = MaxPooling1D(pool_size=5)(layer_conv3)\n        # layer_drop3 = Dropout(0.5)(layer_pool3)\n        # layer_bn3 = BatchNormalization()(layer_drop3)\n\n        layer_bilstm = Bidirectional(GRU(num_filters,return_sequences=False,dropout=0.2,recurrent_dropout=0.2))(layer_bn2)#(layer_drop2)#(layer_pool3)\n\n        # lstm_layer1 = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(layer_bn3)#(pooling_layer2)\n        # lstm_layer2 = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(lstm_layer1)\n    #     layer_drop3 = Dropout(0.5)(layer_bilstm)\n    #     layer_drop3 = Dropout(0.5)(layer_drop3)\n    #     27th Apr\n    #     layer_dense1 = Dense(512,activation='sigmoid')(layer_drop3)\n    #     27th Apr\n\n        layer_drop3 = Dropout(0.5)(layer_bilstm)\n        layer_drop3 = Dropout(0.5)(drop3)\n\n    #     layer_dense1 = Dense(256,activation='relu')(layer_bilstm)\n    #     layer_drop4 = Dropout(0.5)(layer_dense1)\n\n        layer_output = Dense(num_labels,activation='softmax')(layer_drop3)#(layer_dense1)#(layer_drop3)\n\n        # layer_activation = Activation('softmax')(layer_output)\n\n        model_bilstm = Model(inputs=input_sequence, outputs=layer_output)\n        model_bilstm.compile(loss = 'sparse_categorical_crossentropy',metrics=['accuracy'], optimizer=optimizer)\n    return model_bilstm\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createModelCNNBILSTM3(num_filters, kernel_size, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer):\n    with tpu_strategy.scope():\n        print(\"Started execution of function createModelCNNBILSTM3\")\n        print(\"Number of filters: \",num_filters)\n        print(\"Number of kernels: \",kernel_size)\n        print(\"Optimizer is: \",optimizer)\n    #     if optimizer is 'Adam':\n    #         opt='adam'#opt = Adam(learning_rate=0.1)\n    #     elif optimizer is 'SGD':\n    #         opt = SGD(learning_rate=0.001)#, momentum=0.9)#SGD(learning_rate=0.01, momentum=0.9)\n    #     elif optimizer is 'adadelta':\n    #         opt = 'adadelta'\n    #     else:\n    #         raise MyValidationError(\"Invalid optimizer name.\")\n\n        # input_sequence = Input(shape=(9050,), dtype='int32')\n        input_sequence = Input(shape=(maximum_len,), dtype='int32')\n        # layer_embedding = Embedding(len(words_index) + 1,embedding_dimension,weights=[matrix_embeddings],input_length=maximum_len,trainable=False)\n        layer_embedding = Embedding(len(text_tokenizer.word_index) + 1,embedding_dimension,weights=[matrix_embeddings],input_length=maximum_len,trainable=False)\n        sequences_embedded = layer_embedding(input_sequence)\n        layer_conv1 = Conv1D(num_filters, kernel_size, activation='relu')(sequences_embedded)\n        #1layer_pool1 = GlobalMaxPooling1D(pool_size=5)(layer_conv1)\n        #1layer_drop1 = Dropout(0.5)(layer_pool1)\n        #1layer_bn1 = BatchNormalization()(layer_drop1)\n        #1layer_conv2 = Conv1D(num_filters, kernel_size, activation='relu')(layer_bn1)\n        #1layer_pool2 = MaxPooling1D(pool_size=5)(layer_conv2)\n        #1layer_drop2 = Dropout(0.5)(layer_pool2)\n        #1layer_bn2 = BatchNormalization()(layer_drop2)\n        # layer_conv3 = Conv1D(128, 5, activation='relu')(layer_bn2)\n        # layer_pool3 = MaxPooling1D(pool_size=5)(layer_conv3)\n        # layer_drop3 = Dropout(0.5)(layer_pool3)\n        # layer_bn3 = BatchNormalization()(layer_drop3)\n        layer_bilstm = Bidirectional(LSTM(num_filters,return_sequences=True,dropout=0.2,recurrent_dropout=0.2))(layer_conv1)#(layer_pool1)#(layer_bn1)#(layer_bn2)#(layer_drop2)#(layer_pool3)\n        avg_pool = GlobalAveragePooling1D()(layer_bilstm)\n        max_pool = GlobalMaxPooling1D()(layer_bilstm)\n        conc = concatenate([avg_pool, max_pool])\n        # lstm_layer1 = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(layer_bn3)#(pooling_layer2)\n        # lstm_layer2 = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(lstm_layer1)\n    #     layer_drop3 = Dropout(0.5)(conc)#(layer_bigru)\n        layer_dense1 = Dense(embedding_dimension,activation='relu')(conc)#(layer_drop3)\n        layer_drop1 = Dropout(0.5)(layer_dense1)\n        layer_drop1 = Dropout(0.5)(layer_drop1)\n        layer_output = Dense(num_labels,activation='softmax')(layer_drop1)#(layer_dense1)#(layer_drop3)\n        # layer_activation = Activation('softmax')(layer_output)\n        model_bilstm = Model(inputs=input_sequence, outputs=layer_output)\n        model_bilstm.compile(loss = 'sparse_categorical_crossentropy',metrics=['accuracy'], optimizer=optimizer)\n    return model_bilstm\n    ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createModelCNNBIGRU3(num_filters, kernel_size, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer):\n    with tpu_strategy.scope():\n        print(\"Started execution of function createModelCNNBIGRU3\")\n        print(\"Number of filters: \",num_filters)\n        print(\"Number of kernels: \",kernel_size)\n        print(\"Optimizer is: \",optimizer)\n    #     if optimizer is 'Adam':\n    #         opt='adam'#opt = Adam(learning_rate=0.1)\n    #     elif optimizer is 'SGD':\n    #         opt = SGD(learning_rate=0.001)#, momentum=0.9)#SGD(learning_rate=0.01, momentum=0.9)\n    #     elif optimizer is 'adadelta':\n    #         opt = 'adadelta'\n    #     else:\n    #         raise MyValidationError(\"Invalid optimizer name.\")\n\n        # input_sequence = Input(shape=(9050,), dtype='int32')\n        input_sequence = Input(shape=(maximum_len,), dtype='int32')\n        # layer_embedding = Embedding(len(words_index) + 1,embedding_dimension,weights=[matrix_embeddings],input_length=maximum_len,trainable=False)\n        layer_embedding = Embedding(len(text_tokenizer.word_index) + 1,embedding_dimension,weights=[matrix_embeddings],input_length=maximum_len,trainable=False)\n        sequences_embedded = layer_embedding(input_sequence)\n        layer_conv1 = Conv1D(num_filters, kernel_size, activation='relu')(sequences_embedded)\n        #1layer_pool1 = GlobalMaxPooling1D(pool_size=5)(layer_conv1)\n        #1layer_drop1 = Dropout(0.5)(layer_pool1)\n        #1layer_bn1 = BatchNormalization()(layer_drop1)\n        #1layer_conv2 = Conv1D(num_filters, kernel_size, activation='relu')(layer_bn1)\n        #1layer_pool2 = MaxPooling1D(pool_size=5)(layer_conv2)\n        #1layer_drop2 = Dropout(0.5)(layer_pool2)\n        #1layer_bn2 = BatchNormalization()(layer_drop2)\n        # layer_conv3 = Conv1D(128, 5, activation='relu')(layer_bn2)\n        # layer_pool3 = MaxPooling1D(pool_size=5)(layer_conv3)\n        # layer_drop3 = Dropout(0.5)(layer_pool3)\n        # layer_bn3 = BatchNormalization()(layer_drop3)\n        layer_bigru = Bidirectional(GRU(num_filters,return_sequences=True,dropout=0.2,recurrent_dropout=0.2))(layer_conv1)#(layer_pool1)#(layer_bn1)#(layer_bn2)#(layer_drop2)#(layer_pool3)\n        avg_pool = GlobalAveragePooling1D()(layer_bigru)\n        max_pool = GlobalMaxPooling1D()(layer_bigru)\n        conc = concatenate([avg_pool, max_pool])\n        # lstm_layer1 = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(layer_bn3)#(pooling_layer2)\n        # lstm_layer2 = Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2))(lstm_layer1)\n    #     layer_drop3 = Dropout(0.5)(conc)#(layer_bigru)\n        layer_dense1 = Dense(embedding_dimension,activation='relu')(conc)#'relu')(conc)#(layer_drop3)\n        layer_drop1 = Dropout(0.5)(layer_dense1)\n        layer_drop1 = Dropout(0.5)(layer_drop1)\n        layer_output = Dense(num_labels,activation='softmax')(layer_drop1)#(layer_dense1)#'softmax')(layer_dense1)#(layer_drop3)\n        # layer_activation = Activation('softmax')(layer_output)\n        model_bigru = Model(inputs=input_sequence, outputs=layer_output)\n        model_bigru.compile(loss = 'sparse_categorical_crossentropy',metrics=['accuracy'], optimizer=optimizer)\n    return model_bigru\n    ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def createModelCNNBIGRU2(num_filters, kernel_size, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer):\n# def createModelCNNBIGRU2(DLTechniqueName,x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels,batch_size,optimizer,datasetName,embeddingName,class_weights=None):\ndef createModelCNNBIGRU2(num_filters, kernel_size, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer):\n    with tpu_strategy.scope():\n    #     num_filters = 100\n    #     kernel_size = 3\n        print(\"Started execution of function createModelCNNBIGRU2\")\n        print(\"Number of filters: \",num_filters)\n        print(\"Number of kernels: \",kernel_size)\n        print(\"Optimizer is: \",optimizer)\n    #     if optimizer is 'Adam':\n    # #         opt = Adam(learning_rate=0.001)\n    #         opt = 'adam'\n    #     elif optimizer is 'SGD':\n    #         opt = 'sgd'#SGD(learning_rate=0.01)#, momentum=0.9)#SGD(learning_rate=0.01, momentum=0.9)\n    # #         opt = tf.keras.optimizers.legacy.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n    #     elif optimizer is 'adadelta':\n    #         opt = 'adadelta'\n    #     elif optimizer is 'rmsprop':\n    #         opt = tensorflow.keras.optimizers.RMSprop(learning_rate=0.1)\n    #     elif optimizer is 'adagrad':\n    #         opt = 'adagrad'\n    #     elif optimizer is 'nadam':\n    #         opt = tensorflow.keras.optimizers.Nadam(use_ema=True)#'nadam'\n    #     else:\n    #         raise MyValidationError(\"Invalid optimizer name.\")\n\n            # Channel 1\n        input_dim = len(text_tokenizer.word_index)+1#list(matrix_embeddings.shape)[0]#1000\n        output_dim = 300#300\n        input1 = Input(shape=(maximum_len,))\n        embeddding1 = Embedding(\n                                input_dim=input_dim, \n                                output_dim=output_dim, \n                                input_length=maximum_len, \n                                input_shape=(maximum_len, ),\n                                # Assign the embedding weight with word2vec embedding marix\n                                weights = [matrix_embeddings],\n                                # Set the weight to be not trainable (static)\n                                trainable = False)(input1)\n        conv1 = Conv1D(num_filters, kernel_size, activation='relu', \n                       kernel_constraint= MaxNorm( max_value=3, axis=[0,1]))(embeddding1)\n        pool1 = MaxPool1D(pool_size=2, strides=2,padding='same')(conv1)\n        flat1 = Flatten()(pool1)\n        drop1 = Dropout(0.5)(flat1)\n        drop1 = Dropout(0.5)(drop1)\n        dense1 = Dense(10, activation='relu')(drop1)\n        drop1 = Dropout(0.5)(dense1)\n        drop1 = Dropout(0.5)(drop1)\n        out1 = Dense(units=num_labels, activation='softmax')(drop1)\n\n        # Channel 2\n        input2 = Input(shape=(maximum_len,))\n        embeddding2 = Embedding(\n            input_dim=input_dim, \n                                output_dim=output_dim, \n                                input_length=maximum_len, \n                                input_shape=(maximum_len, ),\n                                # Assign the embedding weight with word2vec embedding marix\n                                weights = [matrix_embeddings],\n                                # Set the weight to be not trainable (static)\n                                trainable = False,\n                                mask_zero=True)(input2)\n        gru2 = Bidirectional(GRU(64,dropout=0.2,recurrent_dropout=0.2))(embeddding2)\n        drop2 = Dropout(0.5)(gru2)\n        drop2 = Dropout(0.5)(drop2)\n        out2 = Dense(units=num_labels, activation='softmax')(drop2)\n\n        # Merge\n        merged = concatenate([out1, out2])\n\n        # Interpretation\n        outputs = Dense(units=num_labels, activation='softmax')(merged)\n        model = Model(inputs=[input1,input2], outputs=outputs)\n\n        model.compile( loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    #     model.fit([x_train,x_train], y_train, class_weight = class_weights,validation_data= ([x_test,x_test],y_test,class_weights),epochs=100,\n    #                 callbacks=[tf.keras.callbacks.EarlyStopping(\n    # #                                 monitor='accuracy', patience=10),\n    #                            monitor='val_accuracy', min_delta=0,#0.01, \n    #                                  patience=20,# verbose=2, \n    #                                  mode='max', restore_best_weights=True),#start_from_epoch=15),\n    # #                                     tensorboard_callback,\n    #                                     reduce_lr,\n    #                         checkpoints\n    #                        ],batch_size=batch_size)#50)\n\n    \n    return model","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def createModelCNNBIGRU2(num_filters, kernel_size, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer):\n# def createModelCNNBIGRU2(DLTechniqueName,x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels,batch_size,optimizer,datasetName,embeddingName,class_weights=None):\ndef createModelCNNBILSTM2(num_filters, kernel_size, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer):\n    with tpu_strategy.scope():\n    #     num_filters = 100\n    #     kernel_size = 3\n        print(\"Started execution of function createModelCNNBILSTM2\")\n        print(\"Number of filters: \",num_filters)\n        print(\"Number of kernels: \",kernel_size)\n        print(\"Optimizer is: \",optimizer)\n    #     if optimizer is 'Adam':\n    # #         opt = Adam(learning_rate=0.001)\n    #         opt = 'adam'\n    #     elif optimizer is 'SGD':\n    #         opt = 'sgd'#SGD(learning_rate=0.01)#, momentum=0.9)#SGD(learning_rate=0.01, momentum=0.9)\n    # #         opt = tf.keras.optimizers.legacy.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n    #     elif optimizer is 'adadelta':\n    #         opt = 'adadelta'\n    #     elif optimizer is 'rmsprop':\n    #         opt = tensorflow.keras.optimizers.RMSprop(learning_rate=0.1)\n    #     elif optimizer is 'adagrad':\n    #         opt = 'adagrad'\n    #     elif optimizer is 'nadam':\n    #         opt = tensorflow.keras.optimizers.Nadam(use_ema=True)#'nadam'\n    #     else:\n    #         raise MyValidationError(\"Invalid optimizer name.\")\n        input_dim = len(text_tokenizer.word_index)+1#list(matrix_embeddings.shape)[0]#1000\n        output_dim = 300#300\n        input1 = Input(shape=(maximum_len,))\n        embeddding1 = Embedding(\n                                input_dim=input_dim, \n                                output_dim=output_dim, \n                                input_length=maximum_len, \n                                input_shape=(maximum_len, ),\n                                # Assign the embedding weight with word2vec embedding marix\n                                weights = [matrix_embeddings],\n                                # Set the weight to be not trainable (static)\n                                trainable = False)(input1)\n        conv1 = Conv1D(num_filters, kernel_size, activation='relu', \n                       kernel_constraint= MaxNorm( max_value=3, axis=[0,1]))(embeddding1)\n        pool1 = MaxPool1D(pool_size=2, strides=2,padding='same')(conv1)\n        flat1 = Flatten()(pool1)\n        drop1 = Dropout(0.5)(flat1)\n        drop1 = Dropout(0.5)(drop1)\n        dense1 = Dense(10, activation='relu')(drop1)\n        drop1 = Dropout(0.5)(dense1)\n        drop1 = Dropout(0.5)(drop1)\n        out1 = Dense(units=num_labels, activation='softmax')(drop1)\n\n        # Channel 2\n        input2 = Input(shape=(maximum_len,))\n        embeddding2 = Embedding(\n            input_dim=input_dim, \n                                output_dim=output_dim, \n                                input_length=maximum_len, \n                                input_shape=(maximum_len, ),\n                                # Assign the embedding weight with word2vec embedding marix\n                                weights = [matrix_embeddings],\n                                # Set the weight to be not trainable (static)\n                                trainable = False,\n                                mask_zero=True)(input2)\n        gru2 = Bidirectional(LSTM(64,dropout=0.2,recurrent_dropout=0.2))(embeddding2)\n        drop2 = Dropout(0.5)(gru2)\n        drop2 = Dropout(0.5)(drop2)\n        out2 = Dense(units=num_labels, activation='softmax')(drop2)\n\n        # Merge\n        merged = concatenate([out1, out2])\n\n        # Interpretation\n        outputs = Dense(units=num_labels, activation='softmax')(merged)\n        model = Model(inputs=[input1,input2], outputs=outputs)\n\n        model.compile( loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\n    #     model.fit([x_train,x_train], y_train, class_weight = class_weights,validation_data= ([x_test,x_test],y_test,class_weights),epochs=100,\n    #                 callbacks=[tf.keras.callbacks.EarlyStopping(\n    # #                                 monitor='accuracy', patience=10),\n    #                            monitor='val_accuracy', min_delta=0,#0.01, \n    #                                  patience=20,# verbose=2, \n    #                                  mode='max', restore_best_weights=True),#start_from_epoch=15),\n    # #                                     tensorboard_callback,\n    #                                     reduce_lr,\n    #                         checkpoints\n    #                        ],batch_size=batch_size)#50)\n\n    \n    return model","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def buildAndTrainModel(DLTechniqueName,x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels,batch_size,optimizer,datasetName=None,embeddingName=None,class_weights_train=None,class_weights_test=None,dataFormat=''):\n    print(\"Started function execution of buildAndEvaluateModel\")\n    print(\"Batch size: \",batch_size)\n#     if DLTechniqueName is 'CNNBILSTM':\n#         model = KerasClassifier(build_fn=createModelCNNBILSTM,\n# #                                 epochs=100, #batch_size=batch_size,#64,\n#                                 verbose=True)#False)                                     \n         \n#     elif DLTechniqueName is 'CNNBIGRU':\n#         model = KerasClassifier(build_fn=createModelCNNBIGRU,\n# #                                 epochs=100,# batch_size=batch_size,#64,\n#                                 verbose=True)\n#     elif DLTechniqueName is 'CNNBILSTM2':\n#         model = KerasClassifier(build_fn=createModelCNNBILSTM2,\n# #                                 epochs=100,# batch_size=batch_size,#64,\n#                                 verbose=True)                                     \n#     elif DLTechniqueName is 'CNNBIGRU2':\n#         model = KerasClassifier(build_fn=createModelCNNBIGRU2,\n# #                                 epochs=100,# batch_size=batch_size,#64,\n#                                 verbose=True)                                     \n# #         model = KerasClassifier(build_fn=createModelCNNBIGRU2,\n# #                                 epochs=1, batch_size=batch_size,#64,\n# #                                 verbose=False)\n# #         return createModelCNNBIGRU2(DLTechniqueName,x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels,batch_size,optimizer,datasetName,embeddingName,class_weights)\n#     elif DLTechniqueName is 'CNNBILSTM3':\n#         model = KerasClassifier(build_fn=createModelCNNBILSTM3,\n# #                                 epochs=100,# batch_size=batch_size,#64,\n#                                 verbose=True)\n#     elif DLTechniqueName is 'CNNBIGRU3':\n#         model = KerasClassifier(build_fn=createModelCNNBIGRU3,\n# #                                 epochs=100,# batch_size=batch_size,#64,\n#                                 verbose=True)\n    if DLTechniqueName not in ['CNNBILSTM','CNNBILSTM2','CNNBILSTM3','CNNBIGRU','CNNBIGRU2','CNNBIGRU3']:\n        raise MyValidationError(\"Invalid input hybrid DL model name.\")        \n        \n         \n#     param_grid = dict(num_filters=[100],#128],#[128],#[64,128,256],#,512],#[32, 64, 128, 256],\n#                       kernel_size=[3],#,5],#[3,5],#, 5],\n#                       text_tokenizer=[text_tokenizer],\n#                       embedding_dimension=[embedding_dimension],\n#                       maximum_len=[maximum_len],\n#                       matrix_embeddings=[matrix_embeddings],\n#                      num_labels=[num_labels],\n#                      optimizer=[optimizer])\n#     grid = GridSearchCV(estimator=model, param_grid=param_grid,\n#                                 cv=2, verbose=1)#, n_iter=5)\n#     import datetime\n#     log_dir = \"logs\\\\\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n#     tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=1)\n#     26th Apr\n#     reduce_lr = ReduceLROnPlateau(monitor='val_acc', mode='max',min_delta = 0.01, factor=0.1, patience=2, verbose=1, min_lr=0.0000000000000001)\n#     reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', mode=\"max\",factor=0.1, patience=5, verbose=1, min_lr=0.0000000000000001)#,min_delta=0.1)\n#     model_name = 'model_'+DLTechniqueName+'_'+str(batch_size)+'_'+optimizer+'_'+datasetName+'_'+embeddingName+'_trained.h5'\n#     checkpoints = ModelCheckpoint(model_name, monitor=\"val_accuracy\", mode=\"max\", verbose=True, save_best_only=True)\n    # 26th Apri\n    with tf.device(\"/TPU:0\"):\n        print(\"Started model training:\")\n        \n        reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', mode=\"max\",factor=0.1, patience=8, verbose=0, min_lr=0.0000000000000001)#,min_delta=0.01)\n        num_filters = [100]#,256]\n        num_kernels = [3]#,5],7]\n#         model.compile( loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n        if DLTechniqueName not in ['CNNBILSTM2','CNNBIGRU2']:\n            for num_filter in num_filters:\n                for num_kernel in num_kernels:\n                    model_name = 'model_dataFormat'+dataFormat+'_'+DLTechniqueName+'_batchsize_'+str(batch_size)+'_filters_'+str(num_filter)+'_kernels_'+str(num_kernel)+'_opt_'+optimizer+'_'+datasetName+'_'+embeddingName+'_trained.h5'\n                    checkpoints = ModelCheckpoint(model_name, monitor=\"val_accuracy\",  mode='max',verbose=False, save_best_only=True)\n                    if DLTechniqueName is 'CNNBILSTM':\n                        model = createModelCNNBILSTM(num_filter, num_kernel, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer)\n                    elif DLTechniqueName is 'CNNBILSTM3':\n                        model = createModelCNNBILSTM3(num_filter, num_kernel, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer)                            \n                    elif DLTechniqueName is 'CNNBIGRU':\n                        model = createModelCNNBIGRU(num_filter, num_kernel, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer)\n                    elif DLTechniqueName is 'CNNBIGRU3':\n                        model = createModelCNNBIGRU3(num_filter, num_kernel, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer)                    \n\n                    model.fit(x_train, y_train, class_weight = class_weights_train,validation_data= (x_test,y_test),epochs=100,verbose=0,\n                        callbacks=[tf.keras.callbacks.EarlyStopping(\n        #                                 monitor='accuracy', patience=10),\n                                   monitor='val_accuracy', min_delta=0,#0.01, \n                                         patience=20,# verbose=2, \n                                         mode='max', restore_best_weights=True),#start_from_epoch=15),\n        #                                     tensorboard_callback,\n                                            reduce_lr,\n                                checkpoints\n                               ],batch_size=batch_size)#50)\n                    return model\n        else:\n            print(\"In the other part\")\n            \n            for num_filter in num_filters:\n                for num_kernel in num_kernels:\n                    model_name = 'model_dataFormat'+dataFormat+'_'+DLTechniqueName+'_batchsize_'+str(batch_size)+'_filters_'+str(num_filter)+'_kernels_'+str(num_kernel)+'_opt_'+optimizer+'_'+datasetName+'_'+embeddingName+'_trained.h5'\n                    checkpoints = ModelCheckpoint(model_name, monitor=\"val_accuracy\",  mode='max',verbose=False, save_best_only=True)\n                    \n                    if DLTechniqueName is 'CNNBIGRU2':                        \n                        model = createModelCNNBIGRU2(num_filter, num_kernel, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer)\n                    elif DLTechniqueName is 'CNNBILSTM2':\n                        model = createModelCNNBILSTM2(num_filter, num_kernel, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer)\n                    model.fit([x_train,x_train], y_train, class_weight = class_weights_train,validation_data= ([x_test,x_test],y_test),epochs=100,verbose=0,\n                    callbacks=[tf.keras.callbacks.EarlyStopping(\n    #                                 monitor='accuracy', patience=10),\n                               monitor='val_accuracy', min_delta=0,#0.01, \n                                     patience=20,# verbose=2, \n                                     mode='max', restore_best_weights=True),#start_from_epoch=15),\n    #                                     tensorboard_callback,\n                                        reduce_lr,\n                            checkpoints\n                           ],batch_size=batch_size)#50)\n                    return model\n\n#         if DLTechniqueName is 'CNNBIGRU2':\n#             grid_result = grid.fit(x_train, y_train, validation_data= (x_test,y_test),\n#                             callbacks=[tf.keras.callbacks.EarlyStopping(\n# #                                 monitor='accuracy', patience=10),\n#                                        monitor='val_accuracy', min_delta=0, \n#                                              patience=30, verbose=2, \n#                                              mode='max', restore_best_weights=True),\n# #                                     tensorboard_callback,\n#                                     reduce_lr,\n#                                     checkpoints\n#                                    ])\n#         else:\n#             grid_result = grid.fit(x_train, y_train, validation_data= (x_test,y_test),\n#                                 callbacks=[tf.keras.callbacks.EarlyStopping(\n# #                                     monitor='accuracy', \n# #                                     patience=10,\n# #                                 min_delta = 0,\n#                                 monitor='val_accuracy', min_delta=0,#0.01, \n#                                  patience=20, verbose=2, \n#                                  mode='max', restore_best_weights=True),#start_from_epoch=15),\n# #                                         tensorboard_callback,\n#                                         reduce_lr,\n#                                         checkpoints],\n#                                   batch_size=50\n#                                   )\n    return model\n#     return","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluateModel(grid_result,x_holdout,y_holdout,label_encoder,model=None):\n    #         grid_result.best_estimator_.save('bestCNNBILSTM.h5')\n    if model not in ['CNNBILSTM2','CNNBIGRU2']:\n        y_pred = grid_result.predict(x_holdout)\n    else:\n        y_pred = grid_result.predict([x_holdout,x_holdout])\n    print(\"Dimension of y_pred: \",y_pred.shape)\n    print(\"Dimension of y_holdout: \",y_holdout.shape)\n#     print(\"y_holdout values: \",y_holdout)\n#     print(\"y_pred values:\",y_pred)\n#     print(\"y_pred values sample: \",y_pred.head())\n    y_true = [label_encoder.classes_[x] for x in y_holdout]#.argmax(axis=1)]\n#         y_predicted = [label_encoder.classes_[x] for x in y_pred.argmax(axis=1)]\n    y_predicted = [label_encoder.classes_[x] for x in y_pred.argmax(axis=1)]\n\n    print(classification_report(y_true,y_predicted))\n#     if model is None:\n#         print('Best number of filters: ',grid_result.best_params_['num_filters'])\n#         print('Best kernel size: ',grid_result.best_params_['kernel_size'])\n#         print('Best embedding dimension: ',grid_result.best_params_['embedding_dimension'])\n#         print('Best training accuracy: ',grid_result.best_score_)\n    print('Accuracy :',accuracy_score(y_true,y_predicted))\n    print('F1 score :',f1_score(y_true, y_predicted, average='weighted'))\n    print('Precision score is:',precision_score(y_true,y_predicted,average='weighted'))\n    print('Recall score is:',recall_score(y_true,y_predicted,average='weighted'))\n    \n#     print()\n#     print('Best parameters: ',grid_result.best_params_)\n#             test_accuracy\n    weighted_roc_auc_ovr = roc_auc_score( y_holdout,y_pred,multi_class=\"ovr\",average=\"weighted\",)\n    print(f\"Weighted AUROC score:{weighted_roc_auc_ovr:.2f}\")\n    return\n    \n    ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getData(datasetName,embeddingName,approach): #,DLTechniqueName):\n    print(\"Started function getDataAndTrainModel\")\n#     nltk.download('wordnet')\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension, label_encoder, num_labels, class_weights_train, class_weights_test = readDataAndPreprocess(datasetName,approach)\n    \n    if embeddingName is 'GLOVE':\n        with tf.device(\"/TPU:0\"):\n            matrix_embeddings = getEmbeddingMatrixForGlove(text_tokenizer, embedding_dimension,indexed_embeddings_glove)\n    elif embeddingName is 'WORD2VEC':\n        #\n        matrix_embeddings = getEmbeddingMatrixForWord2Vec(text_tokenizer, embedding_dimension,embeddings_word2vec)\n    elif embeddingName is 'FASTTEXT':\n        #\n        matrix_embeddings = getEmbeddingMatrixForFasttext(text_tokenizer, embedding_dimension,embeddings_fasttext)\n    else:\n        raise MyValidationError(\"Invalid input word embedding technique name.\")        \n#     return buildAndEvaluateModel(DLTechniqueName,x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder)\n    return x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, class_weights_train, class_weights_test","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir nltk_data","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download(\"wordnet\", \"/root/nltk_data\")","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('wordnet')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#  !unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with tf.device(\"/TPU:0\"):\n#     x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, class_weights_train, class_weights_test = getData('MBTI','GLOVE','new')#,'CNNBIGRU')\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_weights_train","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_weights[0:][0:]","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print({i for i in range(5)})","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_weights_new = {i : class_weights[i] for i in range(16)}","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class_weights_new","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = [label_encoder.classes_[x] for x in y_holdout]","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with tf.device(\"/TPU:0\"):\n#     x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, class_weights_train, class_weights_test = getData('MBTI','WORD2VEC','old')\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# newModel = keras.models.load_model('/kaggle/working/model_dataFormatnew_CNNBILSTM_batchsize_50_filters_100_kernels_3_opt_Adam_MBTI_GLOVE_trained.h5')\n# evaluateModel(model,x_holdout,y_holdout,label_encoder,model='CNNBILSTM')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataFormats = ['new']#,'old']\ndatasets = ['MBTI']\ntechniques = ['CNNBILSTM3','CNNBIGRU3']\nbatches = [50,100]\nembeddings = ['WORD2VEC']#['GLOVE','FASTTEXT','WORD2VEC']\noptimizers = ['adam']#,'SGD']","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.Variable(1,dtype='int64')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    for dataFormat in dataFormats:\n        for dataset in datasets:\n            for embedding in embeddings:\n                for technique in techniques:\n                    for num_batch in batches:\n                        for optimizer in optimizers:\n#                             print('Processing started for dataformat: '+dataFormat+' ,dataset: '+dataset+' ,embedding: '+embedding+' ,technique: '+technique+' ,batch_size: '+str(num_batch)+' ,optimizer: '+optimizer)\n                            x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, class_weights_train, class_weights_test = getData(dataset,embedding,dataFormat)\n                            model = buildAndTrainModel(technique,x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, num_batch, 'Adam',dataset,embedding,class_weights_train,class_weights_test,dataFormat)#,'new')\n#                           modelName = dataFormat+'_'+dataset+'_'+embedding+'_'+technique+'_'+str(num_batch)+'_'+optimizer+'.h5'\n                            print('Processing started for dataformat: '+dataFormat+' ,dataset: '+dataset+' ,embedding: '+embedding+' ,technique: '+technique+' ,batch_size: '+str(num_batch)+' ,optimizer: '+optimizer)\n                            evaluateModel(model,x_holdout,y_holdout,label_encoder,model=technique)\n#                             model.save('ISEARdummy.keras')","metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:59:19.218526Z","iopub.execute_input":"2023-05-02T06:59:19.219293Z","iopub.status.idle":"2023-05-02T07:21:04.395523Z","shell.execute_reply.started":"2023-05-02T06:59:19.219257Z","shell.execute_reply":"2023-05-02T07:21:04.393539Z"},"editable":false,"trusted":true},"execution_count":181,"outputs":[{"name":"stdout","text":"Started function getDataAndTrainModel\nStarted function exeuction of readDataAndPreprocess\nStarted execution of function preProcessDataMBTI\nNumber of columns: Index(['type', 'posts'], dtype='object')\nNumber of records: 8675\nNumber of records with personality type assigned:  8675\nClass-wise break-up of personality types:\n type\nINFP    1832\nINFJ    1470\nINTP    1304\nINTJ    1091\nENTP     685\nENFP     675\nISTP     337\nISFP     271\nENTJ     231\nISTJ     205\nENFJ     190\nISFJ     166\nESTP      89\nESFP      48\nESFJ      42\nESTJ      39\nName: count, dtype: int64\nSample tokens \n {'<OOV>': 1, 'like': 2, 'think': 3, 'people': 4, 'would': 5, 'know': 6, 'really': 7, 'thing': 8, 'time': 9, 'feel': 10, 'type': 11, 'make': 12, 'much': 13, 'well': 14, 'friend': 15}\nSample training data padded length is:  785\nShape of training data with padding:  (6072, 785)\nShape of testing data with padding:  (1744, 785)\nShape of holdout data with padding:  (859, 785)\nShape of training labels encoded:  (6072,)\nShape of testing labels encoded:  (1744,)\nShape of holdout labels encoded:  (859,)\nNumber of labels:  16\nStarted execution of function getEmbeddingMatrixForWord2Vec\nStarted creating the embedding matrix.\nShape of the word-embeddings matrix:  (72035, 300)\nStarted function execution of buildAndEvaluateModel\nBatch size:  50\nStarted model training:\nStarted execution of function createModelCNNBILSTM3\nNumber of filters:  100\nNumber of kernels:  3\nOptimizer is:  Adam\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[181], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[38;5;28;01mfor\u001b[39;00m optimizer \u001b[38;5;129;01min\u001b[39;00m optimizers:\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#                             print('Processing started for dataformat: '+dataFormat+' ,dataset: '+dataset+' ,embedding: '+embedding+' ,technique: '+technique+' ,batch_size: '+str(num_batch)+' ,optimizer: '+optimizer)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m                             x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, class_weights_train, class_weights_test \u001b[38;5;241m=\u001b[39m getData(dataset,embedding,dataFormat)\n\u001b[0;32m---> 10\u001b[0m                             model \u001b[38;5;241m=\u001b[39m \u001b[43mbuildAndTrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtechnique\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_holdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_holdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaximum_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dimension\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmatrix_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAdam\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclass_weights_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mclass_weights_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdataFormat\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#,'new')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#                           modelName = dataFormat+'_'+dataset+'_'+embedding+'_'+technique+'_'+str(num_batch)+'_'+optimizer+'.h5'\u001b[39;00m\n\u001b[1;32m     12\u001b[0m                             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessing started for dataformat: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdataFormat\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ,dataset: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mdataset\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ,embedding: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39membedding\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ,technique: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mtechnique\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ,batch_size: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(num_batch)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m ,optimizer: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39moptimizer)\n","Cell \u001b[0;32mIn[161], line 77\u001b[0m, in \u001b[0;36mbuildAndTrainModel\u001b[0;34m(DLTechniqueName, x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension, matrix_embeddings, label_encoder, num_labels, batch_size, optimizer, datasetName, embeddingName, class_weights_train, class_weights_test, dataFormat)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;28;01melif\u001b[39;00m DLTechniqueName \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCNNBIGRU3\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     75\u001b[0m                 model \u001b[38;5;241m=\u001b[39m createModelCNNBIGRU3(num_filter, num_kernel, text_tokenizer, embedding_dimension, maximum_len,matrix_embeddings,num_labels,optimizer)                    \n\u001b[0;32m---> 77\u001b[0m             \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_weights_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m#                                 monitor='accuracy', patience=10),\u001b[39;49;00m\n\u001b[1;32m     80\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#0.01, \u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m# verbose=2, \u001b[39;49;00m\n\u001b[1;32m     82\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#start_from_epoch=15),\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;66;43;03m#                                     tensorboard_callback,\u001b[39;49;00m\n\u001b[1;32m     84\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcheckpoints\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m                       \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#50)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/engine/training.py:1685\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1677\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1678\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1679\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1682\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1683\u001b[0m ):\n\u001b[1;32m   1684\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1685\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1687\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:894\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    891\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 894\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    896\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    897\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:926\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    923\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    924\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    925\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 926\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    927\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    928\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    929\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    930\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:143\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    141\u001b[0m   (concrete_function,\n\u001b[1;32m    142\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1757\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1753\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1759\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1760\u001b[0m     args,\n\u001b[1;32m   1761\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1762\u001b[0m     executing_eagerly)\n\u001b[1;32m   1763\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:381\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    380\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 381\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    388\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    390\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    394\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n","File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"print('test')","metadata":{"execution":{"iopub.status.busy":"2023-05-02T07:21:05.208962Z","iopub.execute_input":"2023-05-02T07:21:05.209869Z","iopub.status.idle":"2023-05-02T07:21:05.215394Z","shell.execute_reply.started":"2023-05-02T07:21:05.209833Z","shell.execute_reply":"2023-05-02T07:21:05.213712Z"},"editable":false,"trusted":true},"execution_count":182,"outputs":[{"name":"stdout","text":"test\n","output_type":"stream"}]},{"cell_type":"code","source":"dataFormats = ['new']#,'old']\ndatasets = ['TEC','ISEAR','MBTI']\ntechniques = ['CNNBILSTM','CNNBIGRU','CNNBILSTM2','CNNBIGRU2','CNNBILSTM3','CNNBIGRU3']\nbatches = [50,100]\nembeddings = ['GLOVE','FASTTEXT','WORD2VEC']\noptimizers = ['adam']#,'SGD']","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    for dataFormat in dataFormats:\n        for dataset in datasets:\n            for embedding in embeddings:\n                for technique in techniques:\n                    for num_batch in batches:\n                        for optimizer in optimizers:\n                            print('Processing started for dataformat: '+dataFormat+' ,dataset: '+dataset+' ,embedding: '+embedding+' ,technique: '+technique+' ,batch_size: '+str(num_batch)+' ,optimizer: '+optimizer)\n                            x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, class_weights_train, class_weights_test = getData(dataset,embedding,dataFormat)\n                            model = buildAndTrainModel(technique,x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, num_batch, 'Adam',dataset,embedding,class_weights_train,class_weights_test,dataFormat)#,'new')\n#                           modelName = dataFormat+'_'+dataset+'_'+embedding+'_'+technique+'_'+str(num_batch)+'_'+optimizer+'.h5'\n                            evaluateModel(model,x_holdout,y_holdout,label_encoder,model=technique)\n#                             model.save('ISEARdummy.keras')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataFormats = ['old']\ndatasets = ['TEC','ISEAR','MBTI']\ntechniques = ['CNNBILSTM','CNNBIGRU','CNNBILSTM2','CNNBIGRU2','CNNBILSTM3','CNNBIGRU3']\nbatches = [50,100]\nembeddings = ['GLOVE','FASTTEXT','WORD2VEC']\noptimizers = ['adam']#,'SGD']","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    for dataFormat in dataFormats:\n        for dataset in datasets:\n            for embedding in embeddings:\n                for technique in techniques:\n                    for num_batch in batches:\n                        for optimizer in optimizers:\n                            print('Processing started for dataformat: '+dataFormat+' ,dataset: '+dataset+' ,embedding: '+embedding+' ,technique: '+technique+' ,batch_size: '+str(num_batch)+' ,optimizer: '+optimizer)\n                            x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, class_weights_train, class_weights_test = getData(dataset,embedding,dataFormat)\n                            model = buildAndTrainModel(technique,x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, num_batch, 'Adam',dataset,embedding,class_weights_train,class_weights_test,dataFormat)#,'new')\n#                           modelName = dataFormat+'_'+dataset+'_'+embedding+'_'+technique+'_'+str(num_batch)+'_'+optimizer+'.h5'\n                            evaluateModel(model,x_holdout,y_holdout,label_encoder,model=technique)\n#                             model.save('ISEARdummy.keras')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n#     for dataFormat in dataFormats:\n#         for dataset in datasets:\n#             for embedding in embeddings:\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, class_weights_train, class_weights_test = getData('ISEAR','GLOVE','new')\n#                 for technique in techniques:\n#                     for num_batch in batches:\n#                         for optimizer in optimizers:\n#                             print('Processing started for dataformat: '+dataFormat+' ,dataset: '+dataset+' ,embedding: '+embedding+' ,technique: '+technique+' ,batch_size: '+str(num_batch)+' ,optimizer: '+optimizer)\n    model = buildAndTrainModel(technique,x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, num_batch, 'Adam',dataset,embedding,class_weights_train,class_weights_test,dataFormat)#,'new')\n#     modelName = dataFormat+'_'+dataset+'_'+embedding+'_'+technique+'_'+str(num_batch)+'_'+optimizer+'.h5'\n    evaluateModel(model,x_holdout,y_holdout,label_encoder,model=technique)\n#     model.save('tecdummy.keras')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    for dataFormat in dataFormats:\n        for dataset in datasets:\n            for embedding in embeddings:\n                x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, class_weights_train, class_weights_test = getData(dataset,embedding,dataFormat)\n                for technique in techniques:\n                    for num_batch in batches:\n                        for optimizer in optimizers:\n                            print('Processing started for dataformat: '+dataFormat+' ,dataset: '+dataset+' ,embedding: '+embedding+' ,technique: '+technique+' ,batch_size: '+str(num_batch)+' ,optimizer: '+optimizer)\n                            model = buildAndTrainModel(technique,x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, num_batch, 'Adam',dataset,embedding,class_weights_train,class_weights_test,dataFormat)#,'new')\n                            modelName = dataFormat+'_'+dataset+'_'+embedding+'_'+technique+'_'+str(num_batch)+'_'+optimizer+'.h5'\n                            evaluateModel(model,x_holdout,y_holdout,label_encoder,model=technique)\n                            model.save(modelName)\n                            ","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluateModel(grid_result_cnnbilstm_mbti_glove_adam_50,x_holdout,y_holdout,label_encoder,model='dd')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    grid_result_cnnbilstm_mbti_glove_adam_100 = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 50, 'Adam','MBTI','GLOVE')","metadata":{"editable":false},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(matrix_embeddings.shape))\nshape1 = list(matrix_embeddings.shape)\nprint(shape1)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    grid_result_cnnbigru2_isear_fasttext_adam = buildAndTrainModel('CNNBIGRU2',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam','ISEAR','FASTTEXT')\n\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluateModel(grid_result_cnnbigru2_isear_fasttext_adam,x_holdout,y_holdout,label_encoder,model='dd')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    grid_result_cnnbigru2_isear_fasttext_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam','ISEAR','FASTTEXT')\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluateModel(grid_result_cnnbilstm_isear_fasttext_adam,x_holdout,y_holdout,label_encoder,model=None)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    grid_result_cnnbigru_isear_fasttext_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam','ISEAR','FASTTEXT')\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluateModel(grid_result_cnnbigru_isear_fasttext_adam,x_holdout,y_holdout,label_encoder,model=None)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    x_train_TEC, x_test_TEC, x_holdout_TEC, y_train_TEC, y_test_TEC, y_holdout_TEC, text_tokenizer_TEC, maximum_len_TEC, embedding_dimension_TEC,matrix_embeddings_TEC, label_encoder_TEC, num_labels_TEC = getData('TEC','FASTTEXT')#,'CNNBIGRU')","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    grid_result_cnnbigru2_tec_fasttext_64_adam = buildAndTrainModel('CNNBIGRU2',x_train_TEC, x_test_TEC, x_holdout_TEC, y_train_TEC, y_test_TEC, y_holdout_TEC, text_tokenizer_TEC, maximum_len_TEC, embedding_dimension_TEC,matrix_embeddings_TEC, label_encoder_TEC, num_labels_TEC, 64, 'Adam','TEC','FASTTEXT')\n\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_fasttext_32_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbilstm_isear_fasttext_32_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_fasttext_32_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbilstm_isear_fasttext_32_sgd,x_holdout,y_holdout,label_encoder)\n\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_fasttext_64_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbilstm_isear_fasttext_64_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_fasttext_64_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbilstm_isear_fasttext_64_sgd,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_fasttext_32_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbigru_isear_fasttext_32_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_fasttext_32_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbigru_isear_fasttext_32_sgd,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_fasttext_64_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbigru_isear_fasttext_64_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_fasttext_64_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbigru_isear_fasttext_64_sgd,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_word2vec_32_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbilstm_isear_word2vec_32_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_word2vec_32_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbilstm_isear_word2vec_32_sgd,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_word2vec_64_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbilstm_isear_word2vec_64_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_word2vec_64_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbilstm_isear_word2vec_64_sgd,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_word2vec_32_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbigru_isear_word2vec_32_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_word2vec_32_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbigru_isear_word2vec_32_sgd,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_word2vec_64_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbigru_isear_word2vec_64_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_word2vec_64_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbigru_isear_word2vec_64_sgd,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_glove_32_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbilstm_isear_glove_32_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_glove_32_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbilstm_isear_glove_32_sgd,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_glove_64_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbilstm_isear_glove_64_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_isear_glove_64_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbilstm_isear_glove_64_sgd,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_glove_32_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbigru_isear_glove_32_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_glove_32_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbigru_isear_glove_32_sgd,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_glove_64_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbigru_isear_glove_64_adam,x_holdout,y_holdout,label_encoder)\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('ISEAR','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_isear_glove_64_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbigru_isear_glove_64_sgd,x_holdout,y_holdout,label_encoder)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_fasttext_32_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbilstm_tec_fasttext_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_fasttext_32_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbilstm_tec_fasttext_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_fasttext_64_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbilstm_tec_fasttext_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_fasttext_64_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbilstm_tec_fasttext_64_sgd,x_holdout,y_holdout,label_encoder)\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_fasttext_32_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbigru_tec_fasttext_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_fasttext_32_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbigru_tec_fasttext_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_fasttext_64_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbigru_tec_fasttext_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_fasttext_64_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbigru_tec_fasttext_64_sgd,x_holdout,y_holdout,label_encoder)\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_word2vec_32_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbilstm_tec_word2vec_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_word2vec_32_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbilstm_tec_word2vec_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_word2vec_64_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbilstm_tec_word2vec_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_word2vec_64_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbilstm_tec_word2vec_64_sgd,x_holdout,y_holdout,label_encoder)\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_word2vec_32_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbigru_tec_word2vec_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_word2vec_32_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbigru_tec_word2vec_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_word2vec_64_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbigru_tec_word2vec_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_word2vec_64_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbigru_tec_word2vec_64_sgd,x_holdout,y_holdout,label_encoder)\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_glove_32_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbilstm_tec_glove_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_glove_32_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbilstm_tec_glove_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_glove_64_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbilstm_tec_glove_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_tec_glove_64_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbilstm_tec_glove_64_sgd,x_holdout,y_holdout,label_encoder)\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_glove_32_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbigru_tec_glove_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_glove_32_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbigru_tec_glove_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_glove_64_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbigru_tec_glove_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_glove_64_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbigru_tec_glove_64_sgd,x_holdout,y_holdout,label_encoder)\n\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('TEC','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_tec_fasttext_64_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbigru_tec_fasttext_64_adam,x_holdout,y_holdout,label_encoder)","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_fasttext_32_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbilstm_MBTI_fasttext_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_fasttext_32_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbilstm_MBTI_fasttext_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_fasttext_64_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbilstm_MBTI_fasttext_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_fasttext_64_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbilstm_MBTI_fasttext_64_sgd,x_holdout,y_holdout,label_encoder)\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_fasttext_32_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbigru_MBTI_fasttext_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_fasttext_32_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbigru_MBTI_fasttext_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_fasttext_64_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbigru_MBTI_fasttext_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','FASTTEXT')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_fasttext_64_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbigru_MBTI_fasttext_64_sgd,x_holdout,y_holdout,label_encoder)\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_word2vec_32_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbilstm_MBTI_word2vec_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_word2vec_32_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbilstm_MBTI_word2vec_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_word2vec_64_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbilstm_MBTI_word2vec_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_word2vec_64_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbilstm_MBTI_word2vec_64_sgd,x_holdout,y_holdout,label_encoder)\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_word2vec_32_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbigru_MBTI_word2vec_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_word2vec_32_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbigru_MBTI_word2vec_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_word2vec_64_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbigru_MBTI_word2vec_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','WORD2VEC')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_word2vec_64_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbigru_MBTI_word2vec_64_sgd,x_holdout,y_holdout,label_encoder)\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_glove_32_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbilstm_MBTI_glove_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_glove_32_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbilstm_MBTI_glove_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_glove_64_adam = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbilstm_MBTI_glove_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbilstm_MBTI_glove_64_sgd = buildAndTrainModel('CNNBILSTM',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbilstm_MBTI_glove_64_sgd,x_holdout,y_holdout,label_encoder)\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_glove_32_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'Adam')\nevaluateModel(grid_result_cnnbigru_MBTI_glove_32_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_glove_32_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 32, 'SGD')\nevaluateModel(grid_result_cnnbigru_MBTI_glove_32_sgd,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_glove_64_adam = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'Adam')\nevaluateModel(grid_result_cnnbigru_MBTI_glove_64_adam,x_holdout,y_holdout,label_encoder)\n\nwith tf.device(\"/TPU:0\"):\n    x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels = getData('MBTI','GLOVE')#,'CNNBIGRU')\n    grid_result_cnnbigru_MBTI_glove_64_sgd = buildAndTrainModel('CNNBIGRU',x_train, x_test, x_holdout, y_train, y_test, y_holdout, text_tokenizer, maximum_len, embedding_dimension,matrix_embeddings, label_encoder, num_labels, 64, 'SGD')\nevaluateModel(grid_result_cnnbigru_MBTI_glove_64_sgd,x_holdout,y_holdout,label_encoder)\n\n","metadata":{"editable":false,"trusted":true},"execution_count":null,"outputs":[]}]}